{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load genFeat_counting_feat.py\n",
    "\n",
    "\"\"\"\n",
    "__file__\n",
    "\n",
    "    genFeat_counting_feat.py\n",
    "\n",
    "__description__\n",
    "\n",
    "    This file generates the following features for each run and fold, and for the entire training and testing set.\n",
    "\n",
    "        1. Basic Counting Features\n",
    "            \n",
    "            1. Count of n-gram in query/title/description\n",
    "\n",
    "            2. Count & Ratio of Digit in query/title/description\n",
    "\n",
    "            3. Count & Ratio of Unique n-gram in query/title/description\n",
    "\n",
    "        2. Intersect Counting Features\n",
    "\n",
    "            1. Count & Ratio of a's n-gram in b's n-gram\n",
    "\n",
    "        3. Intersect Position Features\n",
    "\n",
    "            1. Statistics of Positions of a's n-gram in b's n-gram\n",
    "\n",
    "            2. Statistics of Normalized Positions of a's n-gram in b's n-gram\n",
    "\n",
    "__author__\n",
    "\n",
    "    Chenglong Chen < c.chenglong@gmail.com >\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import ngram\n",
    "import cPickle\n",
    "import numpy as np\n",
    "from nlp_utils import stopwords, english_stemmer, stem_tokens\n",
    "from feat_utils import try_divide, dump_feat_name\n",
    "sys.path.append(\"../\")\n",
    "from param_config import config\n",
    "\n",
    "\n",
    "\n",
    "def get_position_list(target, obs):\n",
    "    \"\"\"\n",
    "        Get the list of positions of obs in target\n",
    "    \"\"\"\n",
    "    pos_of_obs_in_target = [0]\n",
    "    if len(obs) != 0:\n",
    "        pos_of_obs_in_target = [j for j,w in enumerate(obs, start=1) if w in target]\n",
    "        if len(pos_of_obs_in_target) == 0:\n",
    "            pos_of_obs_in_target = [0]\n",
    "    return pos_of_obs_in_target\n",
    "\n",
    "\n",
    "######################\n",
    "## Pre-process data ##\n",
    "######################\n",
    "token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "#token_pattern = r'\\w{1,}'\n",
    "#token_pattern = r\"\\w+\"\n",
    "#token_pattern = r\"[\\w']+\"\n",
    "def preprocess_data(line,\n",
    "                    token_pattern=token_pattern,\n",
    "                    exclude_stopword=config.cooccurrence_word_exclude_stopword,\n",
    "                    encode_digit=False):\n",
    "    token_pattern = re.compile(token_pattern, flags = re.UNICODE | re.LOCALE)\n",
    "    ## tokenize\n",
    "    tokens = [x.lower() for x in token_pattern.findall(line)]\n",
    "    ## stem\n",
    "    tokens_stemmed = stem_tokens(tokens, english_stemmer)\n",
    "    if exclude_stopword:\n",
    "        tokens_stemmed = [x for x in tokens_stemmed if x not in stopwords]\n",
    "    return tokens_stemmed\n",
    "\n",
    "\n",
    "def extract_feat(df):\n",
    "    ## unigram\n",
    "    print \"generate unigram\"\n",
    "    df[\"query_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"query\"]), axis=1))\n",
    "    df[\"title_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"product_title\"]), axis=1))\n",
    "    df[\"description_unigram\"] = list(df.apply(lambda x: preprocess_data(x[\"product_description\"]), axis=1))\n",
    "    ## bigram\n",
    "    print \"generate bigram\"\n",
    "    join_str = \"_\"\n",
    "    df[\"query_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"query_unigram\"], join_str), axis=1))\n",
    "    df[\"title_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"title_unigram\"], join_str), axis=1))\n",
    "    df[\"description_bigram\"] = list(df.apply(lambda x: ngram.getBigram(x[\"description_unigram\"], join_str), axis=1))\n",
    "    ## trigram\n",
    "    print \"generate trigram\"\n",
    "    join_str = \"_\"\n",
    "    df[\"query_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"query_unigram\"], join_str), axis=1))\n",
    "    df[\"title_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"title_unigram\"], join_str), axis=1))\n",
    "    df[\"description_trigram\"] = list(df.apply(lambda x: ngram.getTrigram(x[\"description_unigram\"], join_str), axis=1))\n",
    "\n",
    "\n",
    "    ################################\n",
    "    ## word count and digit count ##\n",
    "    ################################\n",
    "    print \"generate word counting features\"\n",
    "    feat_names = [\"query\", \"title\", \"description\"]\n",
    "    grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "    count_digit = lambda x: sum([1. for w in x if w.isdigit()])\n",
    "    for feat_name in feat_names:\n",
    "        for gram in grams:\n",
    "            ## word count\n",
    "            df[\"count_of_%s_%s\"%(feat_name,gram)] = list(df.apply(lambda x: len(x[feat_name+\"_\"+gram]), axis=1))\n",
    "            df[\"count_of_unique_%s_%s\"%(feat_name,gram)] = list(df.apply(lambda x: len(set(x[feat_name+\"_\"+gram])), axis=1))\n",
    "            df[\"ratio_of_unique_%s_%s\"%(feat_name,gram)] = map(try_divide, df[\"count_of_unique_%s_%s\"%(feat_name,gram)], df[\"count_of_%s_%s\"%(feat_name,gram)])\n",
    "\n",
    "        ## digit count\n",
    "        df[\"count_of_digit_in_%s\"%feat_name] = list(df.apply(lambda x: count_digit(x[feat_name+\"_unigram\"]), axis=1))\n",
    "        df[\"ratio_of_digit_in_%s\"%feat_name] = map(try_divide, df[\"count_of_digit_in_%s\"%feat_name], df[\"count_of_%s_unigram\"%(feat_name)])\n",
    "\n",
    "    ## description missing indicator\n",
    "    df[\"description_missing\"] = list(df.apply(lambda x: int(x[\"description_unigram\"] == \"\"), axis=1))\n",
    "\n",
    "\n",
    "    ##############################\n",
    "    ## intersect word count ##\n",
    "    ##############################\n",
    "    print \"generate intersect word counting features\"\n",
    "    #### unigram\n",
    "    for gram in grams:\n",
    "        for obs_name in feat_names:\n",
    "            for target_name in feat_names:\n",
    "                if target_name != obs_name:\n",
    "                    ## query\n",
    "                    df[\"count_of_%s_%s_in_%s\"%(obs_name,gram,target_name)] = list(df.apply(lambda x: sum([1. for w in x[obs_name+\"_\"+gram] if w in set(x[target_name+\"_\"+gram])]), axis=1))\n",
    "                    df[\"ratio_of_%s_%s_in_%s\"%(obs_name,gram,target_name)] = map(try_divide, df[\"count_of_%s_%s_in_%s\"%(obs_name,gram,target_name)], df[\"count_of_%s_%s\"%(obs_name,gram)])\n",
    "\n",
    "        ## some other feat\n",
    "        df[\"title_%s_in_query_div_query_%s\"%(gram,gram)] = map(try_divide, df[\"count_of_title_%s_in_query\"%gram], df[\"count_of_query_%s\"%gram])\n",
    "        df[\"title_%s_in_query_div_query_%s_in_title\"%(gram,gram)] = map(try_divide, df[\"count_of_title_%s_in_query\"%gram], df[\"count_of_query_%s_in_title\"%gram])\n",
    "        df[\"description_%s_in_query_div_query_%s\"%(gram,gram)] = map(try_divide, df[\"count_of_description_%s_in_query\"%gram], df[\"count_of_query_%s\"%gram])\n",
    "        df[\"description_%s_in_query_div_query_%s_in_description\"%(gram,gram)] = map(try_divide, df[\"count_of_description_%s_in_query\"%gram], df[\"count_of_query_%s_in_description\"%gram])\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    ## intersect word position feat ##\n",
    "    ######################################\n",
    "    print \"generate intersect word position features\"\n",
    "    for gram in grams:\n",
    "        for target_name in feat_names:\n",
    "            for obs_name in feat_names:\n",
    "                if target_name != obs_name:\n",
    "                    pos = list(df.apply(lambda x: get_position_list(x[target_name+\"_\"+gram], obs=x[obs_name+\"_\"+gram]), axis=1))\n",
    "                    ## stats feat on pos\n",
    "                    df[\"pos_of_%s_%s_in_%s_min\" % (obs_name, gram, target_name)] = map(np.min, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_mean\" % (obs_name, gram, target_name)] = map(np.mean, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_median\" % (obs_name, gram, target_name)] = map(np.median, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_max\" % (obs_name, gram, target_name)] = map(np.max, pos)\n",
    "                    df[\"pos_of_%s_%s_in_%s_std\" % (obs_name, gram, target_name)] = map(np.std, pos)\n",
    "                    ## stats feat on normalized_pos\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_min\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_min\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_mean\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_mean\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_median\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_median\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_max\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_max\" % (obs_name, gram, target_name)], df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "                    df[\"normalized_pos_of_%s_%s_in_%s_std\" % (obs_name, gram, target_name)] = map(try_divide, df[\"pos_of_%s_%s_in_%s_std\" % (obs_name, gram, target_name)] , df[\"count_of_%s_%s\" % (obs_name, gram)])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ###############\n",
    "    ## Load Data ##\n",
    "    ###############\n",
    "    ## load data\n",
    "    with open(config.processed_train_data_path, \"rb\") as f:\n",
    "        dfTrain = cPickle.load(f)\n",
    "    with open(config.processed_test_data_path, \"rb\") as f:\n",
    "        dfTest = cPickle.load(f)\n",
    "    ## load pre-defined stratified k-fold index\n",
    "    with open(\"%s/stratifiedKFold.%s.pkl\" % (config.data_folder, config.stratified_label), \"rb\") as f:\n",
    "            skf = cPickle.load(f)\n",
    "\n",
    "    ## file to save feat names\n",
    "    feat_name_file = \"%s/counting.feat_name\" % config.feat_folder\n",
    "\n",
    "\n",
    "    #######################\n",
    "    ## Generate Features ##\n",
    "    #######################\n",
    "    print(\"==================================================\")\n",
    "    print(\"Generate counting features...\")\n",
    "\n",
    "\n",
    "    extract_feat(dfTrain)\n",
    "    feat_names = [\n",
    "        name for name in dfTrain.columns \\\n",
    "            if \"count\" in name \\\n",
    "            or \"ratio\" in name \\\n",
    "            or \"div\" in name \\\n",
    "            or \"pos_of\" in name\n",
    "    ]\n",
    "    feat_names.append(\"description_missing\")\n",
    "\n",
    "\n",
    "    print(\"For cross-validation...\")\n",
    "    for run in range(config.n_runs):\n",
    "        ## use 33% for training and 67 % for validation\n",
    "        ## so we switch trainInd and validInd\n",
    "        for fold, (validInd, trainInd) in enumerate(skf[run]):\n",
    "            print(\"Run: %d, Fold: %d\" % (run+1, fold+1))\n",
    "            path = \"%s/Run%d/Fold%d\" % (config.feat_folder, run+1, fold+1)\n",
    "              \n",
    "            #########################\n",
    "            ## get word count feat ##\n",
    "            #########################\n",
    "            for feat_name in feat_names:\n",
    "                X_train = dfTrain[feat_name].values[trainInd]\n",
    "                X_valid = dfTrain[feat_name].values[validInd]\n",
    "                with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(X_train, f, -1)\n",
    "                with open(\"%s/valid.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "                    cPickle.dump(X_valid, f, -1)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "    print(\"For training and testing...\")\n",
    "    path = \"%s/All\" % config.feat_folder\n",
    "    ## use full version for X_train\n",
    "    extract_feat(dfTest)\n",
    "    for feat_name in feat_names:\n",
    "        X_train = dfTrain[feat_name].values\n",
    "        X_test = dfTest[feat_name].values\n",
    "        with open(\"%s/train.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "            cPickle.dump(X_train, f, -1)\n",
    "        with open(\"%s/test.%s.feat.pkl\" % (path, feat_name), \"wb\") as f:\n",
    "            cPickle.dump(X_test, f, -1)\n",
    "            \n",
    "    ## save feat names\n",
    "    print(\"Feature names are stored in %s\" % feat_name_file)\n",
    "    ## dump feat name\n",
    "    dump_feat_name(feat_names, feat_name_file)\n",
    "\n",
    "    print(\"All Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
