{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train_model.py\n",
    "\n",
    "\"\"\"\n",
    "__file__\n",
    "\n",
    "    train_model.py\n",
    "\n",
    "__description__\n",
    "\n",
    "    This file trains various models.\n",
    "\n",
    "__author__\n",
    "\n",
    "    Chenglong Chen < c.chenglong@gmail.com >\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from scipy.sparse import hstack\n",
    "## sklearn\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import load_svmlight_file, dump_svmlight_file\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.linear_model import Ridge, Lasso, LassoLars, ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "## hyperopt\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "## keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "## cutomized module\n",
    "from model_library_config import feat_folders, feat_names, param_spaces, int_feat\n",
    "sys.path.append(\"../\")\n",
    "from param_config import config\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "from utils import *\n",
    "\n",
    "\n",
    "global trial_counter\n",
    "global log_handler\n",
    "\n",
    "\n",
    "## libfm\n",
    "libfm_exe = \"../../libfm-1.40.windows/libfm.exe\"\n",
    "\n",
    "## rgf\n",
    "call_exe = \"../../rgf1.2/test/call_exe.pl\"\n",
    "rgf_exe = \"../../rgf1.2/bin/rgf.exe\"\n",
    "\n",
    "output_path = \"../../Output\"\n",
    "\n",
    "### global params\n",
    "## you can use bagging to stabilize the predictions\n",
    "bootstrap_ratio = 1\n",
    "bootstrap_replacement = False\n",
    "bagging_size= 1\n",
    "\n",
    "ebc_hard_threshold = False\n",
    "verbose_level = 1\n",
    "\n",
    "\n",
    "#### warpper for hyperopt for logging the training reslut\n",
    "# adopted from\n",
    "#\n",
    "def hyperopt_wrapper(param, feat_folder, feat_name):\n",
    "    global trial_counter\n",
    "    global log_handler\n",
    "    trial_counter += 1\n",
    "\n",
    "    # convert integer feat\n",
    "    for f in int_feat:\n",
    "        if param.has_key(f):\n",
    "            param[f] = int(param[f])\n",
    "\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print \"Trial %d\" % trial_counter\n",
    "\n",
    "    print(\"        Model\")\n",
    "    print(\"              %s\" % feat_name)\n",
    "    print(\"        Param\")\n",
    "    for k,v in sorted(param.items()):\n",
    "        print(\"              %s: %s\" % (k,v))\n",
    "    print(\"        Result\")\n",
    "    print(\"                    Run      Fold      Bag      Kappa      Shape\")\n",
    "\n",
    "    ## evaluate performance\n",
    "    kappa_cv_mean, kappa_cv_std = hyperopt_obj(param, feat_folder, feat_name, trial_counter)\n",
    "\n",
    "    ## log\n",
    "    var_to_log = [\n",
    "        \"%d\" % trial_counter,\n",
    "        \"%.6f\" % kappa_cv_mean, \n",
    "        \"%.6f\" % kappa_cv_std\n",
    "    ]\n",
    "    for k,v in sorted(param.items()):\n",
    "        var_to_log.append(\"%s\" % v)\n",
    "    writer.writerow(var_to_log)\n",
    "    log_handler.flush()\n",
    "\n",
    "    return {'loss': -kappa_cv_mean, 'attachments': {'std': kappa_cv_std}, 'status': STATUS_OK}\n",
    "    \n",
    "\n",
    "#### train CV and final model with a specified parameter setting\n",
    "def hyperopt_obj(param, feat_folder, feat_name, trial_counter):\n",
    "\n",
    "    kappa_cv = np.zeros((config.n_runs, config.n_folds), dtype=float)\n",
    "    for run in range(1,config.n_runs+1):\n",
    "        for fold in range(1,config.n_folds+1):\n",
    "            rng = np.random.RandomState(2015 + 1000 * run + 10 * fold)\n",
    "\n",
    "            #### all the path\n",
    "            path = \"%s/Run%d/Fold%d\" % (feat_folder, run, fold)\n",
    "            save_path = \"%s/Run%d/Fold%d\" % (output_path, run, fold)\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            # feat\n",
    "            feat_train_path = \"%s/train.feat\" % path\n",
    "            feat_valid_path = \"%s/valid.feat\" % path\n",
    "            # weight\n",
    "            weight_train_path = \"%s/train.feat.weight\" % path\n",
    "            weight_valid_path = \"%s/valid.feat.weight\" % path\n",
    "            # info\n",
    "            info_train_path = \"%s/train.info\" % path\n",
    "            info_valid_path = \"%s/valid.info\" % path\n",
    "            # cdf\n",
    "            cdf_valid_path = \"%s/valid.cdf\" % path\n",
    "            # raw prediction path (rank)\n",
    "            raw_pred_valid_path = \"%s/valid.raw.pred.%s_[Id@%d].csv\" % (save_path, feat_name, trial_counter)\n",
    "            rank_pred_valid_path = \"%s/valid.pred.%s_[Id@%d].csv\" % (save_path, feat_name, trial_counter)\n",
    "\n",
    "            ## load feat\n",
    "            X_train, labels_train = load_svmlight_file(feat_train_path)\n",
    "            X_valid, labels_valid = load_svmlight_file(feat_valid_path)\n",
    "            if X_valid.shape[1] < X_train.shape[1]:\n",
    "                X_valid = hstack([X_valid, np.zeros((X_valid.shape[0], X_train.shape[1]-X_valid.shape[1]))])\n",
    "            elif X_valid.shape[1] > X_train.shape[1]:\n",
    "                X_train = hstack([X_train, np.zeros((X_train.shape[0], X_valid.shape[1]-X_train.shape[1]))])\n",
    "            X_train = X_train.tocsr()\n",
    "            X_valid = X_valid.tocsr()\n",
    "            ## load weight\n",
    "            weight_train = np.loadtxt(weight_train_path, dtype=float)\n",
    "            weight_valid = np.loadtxt(weight_valid_path, dtype=float)\n",
    "\n",
    "            ## load valid info\n",
    "            info_train = pd.read_csv(info_train_path)\n",
    "            numTrain = info_train.shape[0]\n",
    "            info_valid = pd.read_csv(info_valid_path)\n",
    "            numValid = info_valid.shape[0]\n",
    "            Y_valid = info_valid[\"median_relevance\"]\n",
    "            ## load cdf\n",
    "            cdf_valid = np.loadtxt(cdf_valid_path, dtype=float)\n",
    "\n",
    "            ## make evalerror func\n",
    "            evalerror_regrank_valid = lambda preds,dtrain: evalerror_regrank_cdf(preds, dtrain, cdf_valid)\n",
    "            evalerror_softmax_valid = lambda preds,dtrain: evalerror_softmax_cdf(preds, dtrain, cdf_valid)\n",
    "            evalerror_softkappa_valid = lambda preds,dtrain: evalerror_softkappa_cdf(preds, dtrain, cdf_valid)\n",
    "            evalerror_ebc_valid = lambda preds,dtrain: evalerror_ebc_cdf(preds, dtrain, cdf_valid, ebc_hard_threshold)\n",
    "            evalerror_cocr_valid = lambda preds,dtrain: evalerror_cocr_cdf(preds, dtrain, cdf_valid)\n",
    "\n",
    "            ##############\n",
    "            ## Training ##\n",
    "            ##############\n",
    "            ## you can use bagging to stabilize the predictions\n",
    "            preds_bagging = np.zeros((numValid, bagging_size), dtype=float)\n",
    "            for n in range(bagging_size):\n",
    "                if bootstrap_replacement:\n",
    "                    sampleSize = int(numTrain*bootstrap_ratio)\n",
    "                    index_base = rng.randint(numTrain, size=sampleSize)\n",
    "                    index_meta = [i for i in range(numTrain) if i not in index_base]\n",
    "                else:\n",
    "                    randnum = rng.uniform(size=numTrain)\n",
    "                    index_base = [i for i in range(numTrain) if randnum[i] < bootstrap_ratio]\n",
    "                    index_meta = [i for i in range(numTrain) if randnum[i] >= bootstrap_ratio]\n",
    "                \n",
    "                if param.has_key(\"booster\"):\n",
    "                    dvalid_base = xgb.DMatrix(X_valid, label=labels_valid, weight=weight_valid)\n",
    "                    dtrain_base = xgb.DMatrix(X_train[index_base], label=labels_train[index_base], weight=weight_train[index_base])\n",
    "                        \n",
    "                    watchlist = []\n",
    "                    if verbose_level >= 2:\n",
    "                        watchlist  = [(dtrain_base, 'train'), (dvalid_base, 'valid')]\n",
    "                    \n",
    "                ## various models\n",
    "                if param[\"task\"] in [\"regression\", \"ranking\"]:\n",
    "                    ## regression & pairwise ranking with xgboost\n",
    "                    bst = xgb.train(param, dtrain_base, param['num_round'], watchlist, feval=evalerror_regrank_valid)\n",
    "                    pred = bst.predict(dvalid_base)\n",
    "\n",
    "                elif param[\"task\"] in [\"softmax\"]:\n",
    "                    ## softmax regression with xgboost\n",
    "                    bst = xgb.train(param, dtrain_base, param['num_round'], watchlist, feval=evalerror_softmax_valid)\n",
    "                    pred = bst.predict(dvalid_base)\n",
    "                    w = np.asarray(range(1,numOfClass+1))\n",
    "                    pred = pred * w[np.newaxis,:]\n",
    "                    pred = np.sum(pred, axis=1)\n",
    "\n",
    "                elif param[\"task\"] in [\"softkappa\"]:\n",
    "                    ## softkappa with xgboost\n",
    "                    obj = lambda preds, dtrain: softkappaObj(preds, dtrain, hess_scale=param['hess_scale'])\n",
    "                    bst = xgb.train(param, dtrain_base, param['num_round'], watchlist, obj=obj, feval=evalerror_softkappa_valid)\n",
    "                    pred = softmax(bst.predict(dvalid_base))\n",
    "                    w = np.asarray(range(1,numOfClass+1))\n",
    "                    pred = pred * w[np.newaxis,:]\n",
    "                    pred = np.sum(pred, axis=1)\n",
    "\n",
    "                elif param[\"task\"]  in [\"ebc\"]:\n",
    "                    ## ebc with xgboost\n",
    "                    obj = lambda preds, dtrain: ebcObj(preds, dtrain)\n",
    "                    bst = xgb.train(param, dtrain_base, param['num_round'], watchlist, obj=obj, feval=evalerror_ebc_valid)\n",
    "                    pred = sigmoid(bst.predict(dvalid_base))\n",
    "                    pred = applyEBCRule(pred, hard_threshold=ebc_hard_threshold)\n",
    "\n",
    "                elif param[\"task\"]  in [\"cocr\"]:\n",
    "                    ## cocr with xgboost\n",
    "                    obj = lambda preds, dtrain: cocrObj(preds, dtrain)\n",
    "                    bst = xgb.train(param, dtrain_base, param['num_round'], watchlist, obj=obj, feval=evalerror_cocr_valid)\n",
    "                    pred = bst.predict(dvalid_base)\n",
    "                    pred = applyCOCRRule(pred)\n",
    "\n",
    "                elif param['task'] == \"reg_skl_rf\":\n",
    "                    ## regression with sklearn random forest regressor\n",
    "                    rf = RandomForestRegressor(n_estimators=param['n_estimators'],\n",
    "                                               max_features=param['max_features'],\n",
    "                                               n_jobs=param['n_jobs'],\n",
    "                                               random_state=param['random_state'])\n",
    "                    rf.fit(X_train[index_base], labels_train[index_base]+1, sample_weight=weight_train[index_base])\n",
    "                    pred = rf.predict(X_valid)\n",
    "\n",
    "                elif param['task'] == \"reg_skl_etr\":\n",
    "                    ## regression with sklearn extra trees regressor\n",
    "                    etr = ExtraTreesRegressor(n_estimators=param['n_estimators'],\n",
    "                                              max_features=param['max_features'],\n",
    "                                              n_jobs=param['n_jobs'],\n",
    "                                              random_state=param['random_state'])\n",
    "                    etr.fit(X_train[index_base], labels_train[index_base]+1, sample_weight=weight_train[index_base])\n",
    "                    pred = etr.predict(X_valid)\n",
    "\n",
    "                elif param['task'] == \"reg_skl_gbm\":\n",
    "                    ## regression with sklearn gradient boosting regressor\n",
    "                    gbm = GradientBoostingRegressor(n_estimators=param['n_estimators'],\n",
    "                                                    max_features=param['max_features'],\n",
    "                                                    learning_rate=param['learning_rate'],\n",
    "                                                    max_depth=param['max_depth'],\n",
    "                                                    subsample=param['subsample'],\n",
    "                                                    random_state=param['random_state'])\n",
    "                    gbm.fit(X_train.toarray()[index_base], labels_train[index_base]+1, sample_weight=weight_train[index_base])\n",
    "                    pred = gbm.predict(X_valid.toarray())\n",
    "\n",
    "                elif param['task'] == \"clf_skl_lr\":\n",
    "                    ## classification with sklearn logistic regression\n",
    "                    lr = LogisticRegression(penalty=\"l2\", dual=True, tol=1e-5,\n",
    "                                            C=param['C'], fit_intercept=True, intercept_scaling=1.0,\n",
    "                                            class_weight='auto', random_state=param['random_state'])\n",
    "                    lr.fit(X_train[index_base], labels_train[index_base]+1)\n",
    "                    pred = lr.predict_proba(X_valid)\n",
    "                    w = np.asarray(range(1,numOfClass+1))\n",
    "                    pred = pred * w[np.newaxis,:]\n",
    "                    pred = np.sum(pred, axis=1)\n",
    "\n",
    "                elif param['task'] == \"reg_skl_svr\":\n",
    "                    ## regression with sklearn support vector regression\n",
    "                    X_train, X_valid = X_train.toarray(), X_valid.toarray()\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train[index_base] = scaler.fit_transform(X_train[index_base])\n",
    "                    X_valid = scaler.transform(X_valid)\n",
    "                    svr = SVR(C=param['C'], gamma=param['gamma'], epsilon=param['epsilon'],\n",
    "                                            degree=param['degree'], kernel=param['kernel'])\n",
    "                    svr.fit(X_train[index_base], labels_train[index_base]+1, sample_weight=weight_train[index_base])\n",
    "                    pred = svr.predict(X_valid)\n",
    "\n",
    "                elif param['task'] == \"reg_skl_ridge\":\n",
    "                    ## regression with sklearn ridge regression\n",
    "                    ridge = Ridge(alpha=param[\"alpha\"], normalize=True)\n",
    "                    ridge.fit(X_train[index_base], labels_train[index_base]+1, sample_weight=weight_train[index_base])\n",
    "                    pred = ridge.predict(X_valid)\n",
    "\n",
    "                elif param['task'] == \"reg_skl_lasso\":\n",
    "                    ## regression with sklearn lasso\n",
    "                    lasso = Lasso(alpha=param[\"alpha\"], normalize=True)\n",
    "                    lasso.fit(X_train[index_base], labels_train[index_base]+1)\n",
    "                    pred = lasso.predict(X_valid)\n",
    "\n",
    "                elif param['task'] == 'reg_libfm':\n",
    "                    ## regression with factorization machine (libfm)\n",
    "                    ## to array\n",
    "                    X_train = X_train.toarray()\n",
    "                    X_valid = X_valid.toarray()\n",
    "\n",
    "                    ## scale\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train[index_base] = scaler.fit_transform(X_train[index_base])\n",
    "                    X_valid = scaler.transform(X_valid)\n",
    "\n",
    "                    ## dump feat\n",
    "                    dump_svmlight_file(X_train[index_base], labels_train[index_base], feat_train_path+\".tmp\")\n",
    "                    dump_svmlight_file(X_valid, labels_valid, feat_valid_path+\".tmp\")\n",
    "\n",
    "                    ## train fm\n",
    "                    cmd = \"%s -task r -train %s -test %s -out %s -dim '1,1,%d' -iter %d > libfm.log\" % ( \\\n",
    "                                libfm_exe, feat_train_path+\".tmp\", feat_valid_path+\".tmp\", raw_pred_valid_path, \\\n",
    "                                param['dim'], param['iter'])\n",
    "                    os.system(cmd)\n",
    "                    os.remove(feat_train_path+\".tmp\")\n",
    "                    os.remove(feat_valid_path+\".tmp\")\n",
    "                    \n",
    "                    ## extract libfm prediction\n",
    "                    pred = np.loadtxt(raw_pred_valid_path, dtype=float)\n",
    "                    ## labels are in [0,1,2,3]\n",
    "                    pred += 1\n",
    "\n",
    "                elif param['task'] == \"reg_keras_dnn\":\n",
    "                    ## regression with keras' deep neural networks\n",
    "                    model = Sequential()\n",
    "                    ## input layer\n",
    "                    model.add(Dropout(param[\"input_dropout\"]))\n",
    "                    ## hidden layers\n",
    "                    first = True\n",
    "                    hidden_layers = param['hidden_layers']\n",
    "                    while hidden_layers > 0:\n",
    "                        if first:\n",
    "                            dim = X_train.shape[1]\n",
    "                            first = False\n",
    "                        else:\n",
    "                            dim = param[\"hidden_units\"]\n",
    "                        model.add(Dense(dim, param[\"hidden_units\"], init='glorot_uniform'))\n",
    "                        if param[\"batch_norm\"]:\n",
    "                            model.add(BatchNormalization((param[\"hidden_units\"],)))\n",
    "                        if param[\"hidden_activation\"] == \"prelu\":\n",
    "                            model.add(PReLU((param[\"hidden_units\"],)))\n",
    "                        else:\n",
    "                            model.add(Activation(param['hidden_activation']))\n",
    "                        model.add(Dropout(param[\"hidden_dropout\"]))\n",
    "                        hidden_layers -= 1\n",
    "\n",
    "                    ## output layer\n",
    "                    model.add(Dense(param[\"hidden_units\"], 1, init='glorot_uniform'))\n",
    "                    model.add(Activation('linear'))\n",
    "\n",
    "                    ## loss\n",
    "                    model.compile(loss='mean_squared_error', optimizer=\"adam\")\n",
    "\n",
    "                    ## to array\n",
    "                    X_train = X_train.toarray()\n",
    "                    X_valid = X_valid.toarray()\n",
    "\n",
    "                    ## scale\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train[index_base] = scaler.fit_transform(X_train[index_base])\n",
    "                    X_valid = scaler.transform(X_valid)\n",
    "\n",
    "                    ## train\n",
    "                    model.fit(X_train[index_base], labels_train[index_base]+1,\n",
    "                                nb_epoch=param['nb_epoch'], batch_size=param['batch_size'],\n",
    "                                validation_split=0, verbose=0)\n",
    "\n",
    "                    ##prediction\n",
    "                    pred = model.predict(X_valid, verbose=0)\n",
    "                    pred.shape = (X_valid.shape[0],)\n",
    "\n",
    "                elif param['task'] == \"reg_rgf\":\n",
    "                    ## regression with regularized greedy forest (rgf)\n",
    "                    ## to array\n",
    "                    X_train, X_valid = X_train.toarray(), X_valid.toarray()\n",
    "\n",
    "                    train_x_fn = feat_train_path+\".x\"\n",
    "                    train_y_fn = feat_train_path+\".y\"\n",
    "                    valid_x_fn = feat_valid_path+\".x\"\n",
    "                    valid_pred_fn = feat_valid_path+\".pred\"\n",
    "\n",
    "                    model_fn_prefix = \"rgf_model\"\n",
    "\n",
    "                    np.savetxt(train_x_fn, X_train[index_base], fmt=\"%.6f\", delimiter='\\t')\n",
    "                    np.savetxt(train_y_fn, labels_train[index_base], fmt=\"%d\", delimiter='\\t')\n",
    "                    np.savetxt(valid_x_fn, X_valid, fmt=\"%.6f\", delimiter='\\t')\n",
    "                    # np.savetxt(valid_y_fn, labels_valid, fmt=\"%d\", delimiter='\\t')\n",
    "\n",
    "\n",
    "                    pars = [\n",
    "                        \"train_x_fn=\",train_x_fn,\"\\n\",\n",
    "                        \"train_y_fn=\",train_y_fn,\"\\n\",\n",
    "                        #\"train_w_fn=\",weight_train_path,\"\\n\",\n",
    "                        \"model_fn_prefix=\",model_fn_prefix,\"\\n\",\n",
    "                        \"reg_L2=\", param['reg_L2'], \"\\n\",\n",
    "                        #\"reg_depth=\", 1.01, \"\\n\",\n",
    "                        \"algorithm=\",\"RGF\",\"\\n\",\n",
    "                        \"loss=\",\"LS\",\"\\n\",\n",
    "                        #\"opt_interval=\", 100, \"\\n\",\n",
    "                        \"valid_interval=\", param['max_leaf_forest'],\"\\n\",\n",
    "                        \"max_leaf_forest=\", param['max_leaf_forest'],\"\\n\",\n",
    "                        \"num_iteration_opt=\", param['num_iteration_opt'], \"\\n\",\n",
    "                        \"num_tree_search=\", param['num_tree_search'], \"\\n\",\n",
    "                        \"min_pop=\", param['min_pop'], \"\\n\",\n",
    "                        \"opt_interval=\", param['opt_interval'], \"\\n\",\n",
    "                        \"opt_stepsize=\", param['opt_stepsize'], \"\\n\",\n",
    "                        \"NormalizeTarget\"\n",
    "                    ]\n",
    "                    pars = \"\".join([str(p) for p in pars])\n",
    "\n",
    "                    rfg_setting_train = \"./rfg_setting_train\"\n",
    "                    with open(rfg_setting_train+\".inp\", \"wb\") as f:\n",
    "                        f.write(pars)\n",
    "\n",
    "                    ## train fm\n",
    "                    cmd = \"perl %s %s train %s >> rgf.log\" % (\n",
    "                            call_exe, rgf_exe, rfg_setting_train)\n",
    "                    #print cmd\n",
    "                    os.system(cmd)\n",
    "\n",
    "\n",
    "                    model_fn = model_fn_prefix + \"-01\" \n",
    "                    pars = [\n",
    "                        \"test_x_fn=\",valid_x_fn,\"\\n\",\n",
    "                        \"model_fn=\", model_fn,\"\\n\",\n",
    "                        \"prediction_fn=\", valid_pred_fn\n",
    "                    ]\n",
    "\n",
    "                    pars = \"\".join([str(p) for p in pars])\n",
    "                    \n",
    "                    rfg_setting_valid = \"./rfg_setting_valid\"\n",
    "                    with open(rfg_setting_valid+\".inp\", \"wb\") as f:\n",
    "                        f.write(pars)\n",
    "                    cmd = \"perl %s %s predict %s >> rgf.log\" % (\n",
    "                            call_exe, rgf_exe, rfg_setting_valid)\n",
    "                    #print cmd\n",
    "                    os.system(cmd)\n",
    "\n",
    "                    pred = np.loadtxt(valid_pred_fn, dtype=float)\n",
    "\n",
    "                ## weighted averageing over different models\n",
    "                pred_valid = pred\n",
    "                ## this bagging iteration\n",
    "                preds_bagging[:,n] = pred_valid\n",
    "                pred_raw = np.mean(preds_bagging[:,:(n+1)], axis=1)\n",
    "                pred_rank = pred_raw.argsort().argsort()\n",
    "                pred_score, cutoff = getScore(pred_rank, cdf_valid, valid=True)\n",
    "                kappa_valid = quadratic_weighted_kappa(pred_score, Y_valid)\n",
    "                if (n+1) != bagging_size:\n",
    "                    print(\"              {:>3}   {:>3}   {:>3}   {:>6}   {} x {}\".format(\n",
    "                                run, fold, n+1, np.round(kappa_valid,6), X_train.shape[0], X_train.shape[1]))\n",
    "                else:\n",
    "                    print(\"                    {:>3}       {:>3}      {:>3}    {:>8}  {} x {}\".format(\n",
    "                                run, fold, n+1, np.round(kappa_valid,6), X_train.shape[0], X_train.shape[1]))\n",
    "            kappa_cv[run-1,fold-1] = kappa_valid\n",
    "            ## save this prediction\n",
    "            dfPred = pd.DataFrame({\"target\": Y_valid, \"prediction\": pred_raw})\n",
    "            dfPred.to_csv(raw_pred_valid_path, index=False, header=True,\n",
    "                         columns=[\"target\", \"prediction\"])\n",
    "            ## save this prediction\n",
    "            dfPred = pd.DataFrame({\"target\": Y_valid, \"prediction\": pred_rank})\n",
    "            dfPred.to_csv(rank_pred_valid_path, index=False, header=True,\n",
    "                         columns=[\"target\", \"prediction\"])\n",
    "\n",
    "    kappa_cv_mean = np.mean(kappa_cv)\n",
    "    kappa_cv_std = np.std(kappa_cv)\n",
    "    if verbose_level >= 1:\n",
    "        print(\"              Mean: %.6f\" % kappa_cv_mean)\n",
    "        print(\"              Std: %.6f\" % kappa_cv_std)\n",
    "\n",
    "\n",
    "    ####################\n",
    "    #### Retraining ####\n",
    "    ####################\n",
    "    #### all the path\n",
    "    path = \"%s/All\" % (feat_folder)\n",
    "    save_path = \"%s/All\" % output_path\n",
    "    subm_path = \"%s/Subm\" % output_path\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    if not os.path.exists(subm_path):\n",
    "        os.makedirs(subm_path)\n",
    "    # feat\n",
    "    feat_train_path = \"%s/train.feat\" % path\n",
    "    feat_test_path = \"%s/test.feat\" % path\n",
    "    # weight\n",
    "    weight_train_path = \"%s/train.feat.weight\" % path\n",
    "    # info\n",
    "    info_train_path = \"%s/train.info\" % path\n",
    "    info_test_path = \"%s/test.info\" % path\n",
    "    # cdf\n",
    "    cdf_test_path = \"%s/test.cdf\" % path\n",
    "    # raw prediction path (rank)\n",
    "    raw_pred_test_path = \"%s/test.raw.pred.%s_[Id@%d].csv\" % (save_path, feat_name, trial_counter)\n",
    "    rank_pred_test_path = \"%s/test.pred.%s_[Id@%d].csv\" % (save_path, feat_name, trial_counter)\n",
    "    # submission path (relevance as in [1,2,3,4])\n",
    "    subm_path = \"%s/test.pred.%s_[Id@%d]_[Mean%.6f]_[Std%.6f].csv\" % (subm_path, feat_name, trial_counter, kappa_cv_mean, kappa_cv_std)\n",
    "\n",
    "    #### load data\n",
    "    ## load feat\n",
    "    X_train, labels_train = load_svmlight_file(feat_train_path)\n",
    "    X_test, labels_test = load_svmlight_file(feat_test_path)\n",
    "    if X_test.shape[1] < X_train.shape[1]:\n",
    "        X_test = hstack([X_test, np.zeros((X_test.shape[0], X_train.shape[1]-X_test.shape[1]))])\n",
    "    elif X_test.shape[1] > X_train.shape[1]:\n",
    "        X_train = hstack([X_train, np.zeros((X_train.shape[0], X_test.shape[1]-X_train.shape[1]))])\n",
    "    X_train = X_train.tocsr()\n",
    "    X_test = X_test.tocsr()\n",
    "    ## load train weight\n",
    "    weight_train = np.loadtxt(weight_train_path, dtype=float)\n",
    "    ## load test info\n",
    "    info_train = pd.read_csv(info_train_path)\n",
    "    numTrain = info_train.shape[0]\n",
    "    info_test = pd.read_csv(info_test_path)\n",
    "    numTest = info_test.shape[0]\n",
    "    id_test = info_test[\"id\"]\n",
    "    \n",
    "    ## load cdf\n",
    "    cdf_test = np.loadtxt(cdf_test_path, dtype=float)  \n",
    "    ##\n",
    "    evalerror_regrank_test = lambda preds,dtrain: evalerror_regrank_cdf(preds, dtrain, cdf_test)\n",
    "    evalerror_softmax_test = lambda preds,dtrain: evalerror_softmax_cdf(preds, dtrain, cdf_test)\n",
    "    evalerror_softkappa_test = lambda preds,dtrain: evalerror_softkappa_cdf(preds, dtrain, cdf_test)\n",
    "    evalerror_ebc_test = lambda preds,dtrain: evalerror_ebc_cdf(preds, dtrain, cdf_test, ebc_hard_threshold)\n",
    "    evalerror_cocr_test = lambda preds,dtrain: evalerror_cocr_cdf(preds, dtrain, cdf_test)\n",
    "                    \n",
    "    ## bagging\n",
    "    preds_bagging = np.zeros((numTest, bagging_size), dtype=float)\n",
    "    for n in range(bagging_size):\n",
    "        if bootstrap_replacement:\n",
    "            sampleSize = int(numTrain*bootstrap_ratio)\n",
    "            #index_meta = rng.randint(numTrain, size=sampleSize)\n",
    "            #index_base = [i for i in range(numTrain) if i not in index_meta]\n",
    "            index_base = rng.randint(numTrain, size=sampleSize)\n",
    "            index_meta = [i for i in range(numTrain) if i not in index_base]\n",
    "        else:\n",
    "            randnum = rng.uniform(size=numTrain)\n",
    "            index_base = [i for i in range(numTrain) if randnum[i] < bootstrap_ratio]\n",
    "            index_meta = [i for i in range(numTrain) if randnum[i] >= bootstrap_ratio]\n",
    " \n",
    "        if param.has_key(\"booster\"):\n",
    "            dtest = xgb.DMatrix(X_test, label=labels_test)\n",
    "            dtrain = xgb.DMatrix(X_train[index_base], label=labels_train[index_base], weight=weight_train[index_base])\n",
    "                \n",
    "            watchlist = []\n",
    "            if verbose_level >= 2:\n",
    "                watchlist  = [(dtrain, 'train')]\n",
    "                    \n",
    "        ## train\n",
    "        if param[\"task\"] in [\"regression\", \"ranking\"]:\n",
    "            bst = xgb.train(param, dtrain, param['num_round'], watchlist, feval=evalerror_regrank_test)\n",
    "            pred = bst.predict(dtest)\n",
    "\n",
    "        elif param[\"task\"] in [\"softmax\"]:\n",
    "            bst = xgb.train(param, dtrain, param['num_round'], watchlist, feval=evalerror_softmax_test)\n",
    "            pred = bst.predict(dtest)\n",
    "            w = np.asarray(range(1,numOfClass+1))\n",
    "            pred = pred * w[np.newaxis,:]\n",
    "            pred = np.sum(pred, axis=1)\n",
    "\n",
    "        elif param[\"task\"] in [\"softkappa\"]:\n",
    "            obj = lambda preds, dtrain: softkappaObj(preds, dtrain, hess_scale=param['hess_scale'])\n",
    "            bst = xgb.train(param, dtrain, param['num_round'], watchlist, obj=obj, feval=evalerror_softkappa_test)\n",
    "            pred = softmax(bst.predict(dtest))\n",
    "            w = np.asarray(range(1,numOfClass+1))\n",
    "            pred = pred * w[np.newaxis,:]\n",
    "            pred = np.sum(pred, axis=1)\n",
    "\n",
    "        elif param[\"task\"]  in [\"ebc\"]:\n",
    "            obj = lambda preds, dtrain: ebcObj(preds, dtrain)\n",
    "            bst = xgb.train(param, dtrain, param['num_round'], watchlist, obj=obj, feval=evalerror_ebc_test)\n",
    "            pred = sigmoid(bst.predict(dtest))\n",
    "            pred = applyEBCRule(pred, hard_threshold=ebc_hard_threshold)\n",
    "\n",
    "        elif param[\"task\"]  in [\"cocr\"]:\n",
    "            obj = lambda preds, dtrain: cocrObj(preds, dtrain)\n",
    "            bst = xgb.train(param, dtrain, param['num_round'], watchlist, obj=obj, feval=evalerror_cocr_test)\n",
    "            pred = bst.predict(dtest)\n",
    "            pred = applyCOCRRule(pred)\n",
    "\n",
    "        elif param['task'] == \"reg_skl_rf\":\n",
    "            ## random forest regressor\n",
    "            rf = RandomForestRegressor(n_estimators=param['n_estimators'],\n",
    "                                       max_features=param['max_features'],\n",
    "                                       n_jobs=param['n_jobs'],\n",
    "                                       random_state=param['random_state'])\n",
    "            rf.fit(X_train[index_base], labels_train[index_base]+1, sample_weight=weight_train[index_base])\n",
    "            pred = rf.predict(X_test)\n",
    "\n",
    "        elif param['task'] == \"reg_skl_etr\":\n",
    "            ## extra trees regressor\n",
    "            etr = ExtraTreesRegressor(n_estimators=param['n_estimators'],\n",
    "                                      max_features=param['max_features'],\n",
    "                                      n_jobs=param['n_jobs'],\n",
    "                                      random_state=param['random_state'])\n",
    "            etr.fit(X_train[index_base], labels_train[index_base]+1, sample_weight=weight_train[index_base])\n",
    "            pred = etr.predict(X_test)\n",
    "\n",
    "        elif param['task'] == \"reg_skl_gbm\":\n",
    "            ## gradient boosting regressor\n",
    "            gbm = GradientBoostingRegressor(n_estimators=param['n_estimators'],\n",
    "                                            max_features=param['max_features'],\n",
    "                                            learning_rate=param['learning_rate'],\n",
    "                                            max_depth=param['max_depth'],\n",
    "                                            subsample=param['subsample'],\n",
    "                                            random_state=param['random_state'])\n",
    "            gbm.fit(X_train.toarray()[index_base], labels_train[index_base]+1, sample_weight=weight_train[index_base])\n",
    "            pred = gbm.predict(X_test.toarray())\n",
    "\n",
    "        elif param['task'] == \"clf_skl_lr\":\n",
    "            lr = LogisticRegression(penalty=\"l2\", dual=True, tol=1e-5,\n",
    "                                    C=param['C'], fit_intercept=True, intercept_scaling=1.0,\n",
    "                                    class_weight='auto', random_state=param['random_state'])\n",
    "            lr.fit(X_train[index_base], labels_train[index_base]+1)\n",
    "            pred = lr.predict_proba(X_test)\n",
    "            w = np.asarray(range(1,numOfClass+1))\n",
    "            pred = pred * w[np.newaxis,:]\n",
    "            pred = np.sum(pred, axis=1)\n",
    "\n",
    "        elif param['task'] == \"reg_skl_svr\":\n",
    "            ## regression with sklearn support vector regression\n",
    "            X_train, X_test = X_train.toarray(), X_test.toarray()\n",
    "            scaler = StandardScaler()\n",
    "            X_train[index_base] = scaler.fit_transform(X_train[index_base])\n",
    "            X_test = scaler.transform(X_test)\n",
    "            svr = SVR(C=param['C'], gamma=param['gamma'], epsilon=param['epsilon'],\n",
    "                                    degree=param['degree'], kernel=param['kernel'])\n",
    "            svr.fit(X_train[index_base], labels_train[index_base]+1, sample_weight=weight_train[index_base])\n",
    "            pred = svr.predict(X_test)\n",
    "\n",
    "        elif param['task'] == \"reg_skl_ridge\":\n",
    "            ridge = Ridge(alpha=param[\"alpha\"], normalize=True)\n",
    "            ridge.fit(X_train[index_base], labels_train[index_base]+1, sample_weight=weight_train[index_base])\n",
    "            pred = ridge.predict(X_test)\n",
    "\n",
    "        elif param['task'] == \"reg_skl_lasso\":\n",
    "            lasso = Lasso(alpha=param[\"alpha\"], normalize=True)\n",
    "            lasso.fit(X_train[index_base], labels_train[index_base]+1)\n",
    "            pred = lasso.predict(X_test)\n",
    "\n",
    "        elif param['task'] == 'reg_libfm':\n",
    "            ## to array\n",
    "            X_train, X_test = X_train.toarray(), X_test.toarray()\n",
    "\n",
    "            ## scale\n",
    "            scaler = StandardScaler()\n",
    "            X_train[index_base] = scaler.fit_transform(X_train[index_base])\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            ## dump feat\n",
    "            dump_svmlight_file(X_train[index_base], labels_train[index_base], feat_train_path+\".tmp\")\n",
    "            dump_svmlight_file(X_test, labels_test, feat_test_path+\".tmp\")\n",
    "\n",
    "            ## train fm\n",
    "            cmd = \"%s -task r -train %s -test %s -out %s -dim '1,1,%d' -iter %d > libfm.log\" % ( \\\n",
    "                        libfm_exe, feat_train_path+\".tmp\", feat_test_path+\".tmp\", raw_pred_test_path, \\\n",
    "                        param['dim'], param['iter'])\n",
    "            os.system(cmd)\n",
    "            os.remove(feat_train_path+\".tmp\")\n",
    "            os.remove(feat_test_path+\".tmp\")\n",
    "            \n",
    "            ## extract libfm prediction\n",
    "            pred = np.loadtxt(raw_pred_test_path, dtype=float)\n",
    "            ## labels are in [0,1,2,3]\n",
    "            pred += 1\n",
    "\n",
    "        elif param['task'] == \"reg_keras_dnn\":\n",
    "            ## regression with keras deep neural networks\n",
    "            model = Sequential()\n",
    "            ## input layer\n",
    "            model.add(Dropout(param[\"input_dropout\"]))\n",
    "            ## hidden layers\n",
    "            first = True\n",
    "            hidden_layers = param['hidden_layers']\n",
    "            while hidden_layers > 0:\n",
    "                if first:\n",
    "                    dim = X_train.shape[1]\n",
    "                    first = False\n",
    "                else:\n",
    "                    dim = param[\"hidden_units\"]\n",
    "                model.add(Dense(dim, param[\"hidden_units\"], init='glorot_uniform'))\n",
    "                if param[\"batch_norm\"]:\n",
    "                    model.add(BatchNormalization((param[\"hidden_units\"],)))\n",
    "                if param[\"hidden_activation\"] == \"prelu\":\n",
    "                    model.add(PReLU((param[\"hidden_units\"],)))\n",
    "                else:\n",
    "                    model.add(Activation(param['hidden_activation']))\n",
    "                model.add(Dropout(param[\"hidden_dropout\"]))\n",
    "                hidden_layers -= 1\n",
    "\n",
    "            ## output layer\n",
    "            model.add(Dense(param[\"hidden_units\"], 1, init='glorot_uniform'))\n",
    "            model.add(Activation('linear'))\n",
    "\n",
    "            ## loss\n",
    "            model.compile(loss='mean_squared_error', optimizer=\"adam\")\n",
    "\n",
    "            ## to array\n",
    "            X_train = X_train.toarray()\n",
    "            X_test = X_test.toarray()\n",
    "\n",
    "            ## scale\n",
    "            scaler = StandardScaler()\n",
    "            X_train[index_base] = scaler.fit_transform(X_train[index_base])\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            ## train\n",
    "            model.fit(X_train[index_base], labels_train[index_base]+1,\n",
    "                        nb_epoch=param['nb_epoch'], batch_size=param['batch_size'], verbose=0)\n",
    "\n",
    "            ##prediction\n",
    "            pred = model.predict(X_test, verbose=0)\n",
    "            pred.shape = (X_test.shape[0],)\n",
    "\n",
    "        elif param['task'] == \"reg_rgf\":\n",
    "            ## to array\n",
    "            X_train, X_test = X_train.toarray(), X_test.toarray()\n",
    "\n",
    "            train_x_fn = feat_train_path+\".x\"\n",
    "            train_y_fn = feat_train_path+\".y\"\n",
    "            test_x_fn = feat_test_path+\".x\"\n",
    "            test_pred_fn = feat_test_path+\".pred\"\n",
    "\n",
    "            model_fn_prefix = \"rgf_model\"\n",
    "\n",
    "            np.savetxt(train_x_fn, X_train[index_base], fmt=\"%.6f\", delimiter='\\t')\n",
    "            np.savetxt(train_y_fn, labels_train[index_base], fmt=\"%d\", delimiter='\\t')\n",
    "            np.savetxt(test_x_fn, X_test, fmt=\"%.6f\", delimiter='\\t')\n",
    "            # np.savetxt(valid_y_fn, labels_valid, fmt=\"%d\", delimiter='\\t')\n",
    "\n",
    "\n",
    "            pars = [\n",
    "                \"train_x_fn=\",train_x_fn,\"\\n\",\n",
    "                \"train_y_fn=\",train_y_fn,\"\\n\",\n",
    "                #\"train_w_fn=\",weight_train_path,\"\\n\",\n",
    "                \"model_fn_prefix=\",model_fn_prefix,\"\\n\",\n",
    "                \"reg_L2=\", param['reg_L2'], \"\\n\",\n",
    "                #\"reg_depth=\", 1.01, \"\\n\",\n",
    "                \"algorithm=\",\"RGF\",\"\\n\",\n",
    "                \"loss=\",\"LS\",\"\\n\",\n",
    "                \"test_interval=\", param['max_leaf_forest'],\"\\n\",\n",
    "                \"max_leaf_forest=\", param['max_leaf_forest'],\"\\n\",\n",
    "                \"num_iteration_opt=\", param['num_iteration_opt'], \"\\n\",\n",
    "                \"num_tree_search=\", param['num_tree_search'], \"\\n\",\n",
    "                \"min_pop=\", param['min_pop'], \"\\n\",\n",
    "                \"opt_interval=\", param['opt_interval'], \"\\n\",\n",
    "                \"opt_stepsize=\", param['opt_stepsize'], \"\\n\",\n",
    "                \"NormalizeTarget\"\n",
    "            ]\n",
    "            pars = \"\".join([str(p) for p in pars])\n",
    "\n",
    "            rfg_setting_train = \"./rfg_setting_train\"\n",
    "            with open(rfg_setting_train+\".inp\", \"wb\") as f:\n",
    "                f.write(pars)\n",
    "\n",
    "            ## train fm\n",
    "            cmd = \"perl %s %s train %s >> rgf.log\" % (\n",
    "                    call_exe, rgf_exe, rfg_setting_train)\n",
    "            #print cmd\n",
    "            os.system(cmd)\n",
    "\n",
    "\n",
    "            model_fn = model_fn_prefix + \"-01\" \n",
    "            pars = [\n",
    "                \"test_x_fn=\",test_x_fn,\"\\n\",\n",
    "                \"model_fn=\", model_fn,\"\\n\",\n",
    "                \"prediction_fn=\", test_pred_fn\n",
    "            ]\n",
    "\n",
    "            pars = \"\".join([str(p) for p in pars])\n",
    "            \n",
    "            rfg_setting_test = \"./rfg_setting_test\"\n",
    "            with open(rfg_setting_test+\".inp\", \"wb\") as f:\n",
    "                f.write(pars)\n",
    "            cmd = \"perl %s %s predict %s >> rgf.log\" % (\n",
    "                    call_exe, rgf_exe, rfg_setting_test)\n",
    "            #print cmd\n",
    "            os.system(cmd)\n",
    "\n",
    "            pred = np.loadtxt(test_pred_fn, dtype=float)\n",
    "\n",
    "        ## weighted averageing over different models\n",
    "        pred_test = pred\n",
    "        preds_bagging[:,n] = pred_test\n",
    "    pred_raw = np.mean(preds_bagging, axis=1)\n",
    "    pred_rank = pred_raw.argsort().argsort()\n",
    "    #\n",
    "    ## write\n",
    "    output = pd.DataFrame({\"id\": id_test, \"prediction\": pred_raw})    \n",
    "    output.to_csv(raw_pred_test_path, index=False)\n",
    "\n",
    "    ## write\n",
    "    output = pd.DataFrame({\"id\": id_test, \"prediction\": pred_rank})    \n",
    "    output.to_csv(rank_pred_test_path, index=False)\n",
    "\n",
    "    ## write score\n",
    "    pred_score = getScore(pred, cdf_test)\n",
    "    output = pd.DataFrame({\"id\": id_test, \"prediction\": pred_score})    \n",
    "    output.to_csv(subm_path, index=False)\n",
    "    #\"\"\"\n",
    "        \n",
    "    return kappa_cv_mean, kappa_cv_std\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "####################\n",
    "## Model Buliding ##\n",
    "####################\n",
    "\n",
    "def check_model(models, feat_name):\n",
    "    if models == \"all\":\n",
    "        return True\n",
    "    for model in models:\n",
    "        if model in feat_name:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    specified_models = sys.argv[1:]\n",
    "    if len(specified_models) == 0:\n",
    "        print(\"You have to specify which model to train.\\n\")\n",
    "        print(\"Usage: python ./train_model_library_lsa.py model1 model2 model3 ...\\n\")\n",
    "        print(\"Example: python ./train_model_library_lsa.py reg_skl_ridge reg_skl_lasso reg_skl_svr\\n\")\n",
    "        print(\"See model_library_config_lsa.py for a list of available models (i.e., Model@model_name)\")\n",
    "        sys.exit()\n",
    "    log_path = \"%s/Log\" % output_path\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "    for feat_name, feat_folder in zip(feat_names, feat_folders):\n",
    "        if not check_model(specified_models, feat_name):\n",
    "            continue\n",
    "        param_space = param_spaces[feat_name]\n",
    "        #\"\"\"\n",
    "\n",
    "        log_file = \"%s/%s_hyperopt.log\" % (log_path, feat_name)\n",
    "        log_handler = open( log_file, 'wb' )\n",
    "        writer = csv.writer( log_handler )\n",
    "        headers = [ 'trial_counter', 'kappa_mean', 'kappa_std' ]\n",
    "        for k,v in sorted(param_space.items()):\n",
    "            headers.append(k)\n",
    "        writer.writerow( headers )\n",
    "        log_handler.flush()\n",
    "        \n",
    "        print(\"************************************************************\")\n",
    "        print(\"Search for the best params\")\n",
    "        #global trial_counter\n",
    "        trial_counter = 0\n",
    "        trials = Trials()\n",
    "        objective = lambda p: hyperopt_wrapper(p, feat_folder, feat_name)\n",
    "        best_params = fmin(objective, param_space, algo=tpe.suggest,\n",
    "                           trials=trials, max_evals=param_space[\"max_evals\"])\n",
    "        for f in int_feat:\n",
    "            if best_params.has_key(f):\n",
    "                best_params[f] = int(best_params[f])\n",
    "        print(\"************************************************************\")\n",
    "        print(\"Best params\")\n",
    "        for k,v in best_params.items():\n",
    "            print \"        %s: %s\" % (k,v)\n",
    "        trial_kappas = -np.asarray(trials.losses(), dtype=float)\n",
    "        best_kappa_mean = max(trial_kappas)\n",
    "        ind = np.where(trial_kappas == best_kappa_mean)[0][0]\n",
    "        best_kappa_std = trials.trial_attachments(trials.trials[ind])['std']\n",
    "        print(\"Kappa stats\")\n",
    "        print(\"        Mean: %.6f\\n        Std: %.6f\" % (best_kappa_mean, best_kappa_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
